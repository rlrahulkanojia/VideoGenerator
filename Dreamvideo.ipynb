{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9d844-dffd-44a6-89d3-af3f787867d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96844d8-d656-408c-bb7e-8b85ec24a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install modelscope\n",
    "# from modelscope.hub.snapshot_download import snapshot_download\n",
    "# model_dir = snapshot_download('iic/dreamvideo-t2v', cache_dir='models/')\n",
    "# !mv ./models/iic/dreamvideo-t2v/* ./models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbaec06-dd9b-453f-803d-3e9a133798b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "api_key = \"sk-proj-pKaugqFY1WJ19hSq71lrT3BlbkFJVxwwYAMxPfQGX7G3eUOi\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=api_key\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ab4d70-05d9-43f5-b377-301f8866b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_image(prompt, filename):\n",
    "    \"\"\"\n",
    "    Generates an image from a prompt using OpenAI's API and saves it locally.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The prompt to generate the image from.\n",
    "    - filename (str): The local filename to save the image.\n",
    "    \"\"\"\n",
    "    # Call the OpenAI API to generate the image\n",
    "    response = OpenAI().images.generate(\n",
    "      model=\"dall-e-3\",\n",
    "      prompt=prompt,\n",
    "      size=\"1024x1024\",\n",
    "      quality=\"standard\",\n",
    "      n=1,\n",
    "    )\n",
    "\n",
    "    # Get the image URL from the response\n",
    "    image_url = dict(response)['data'][0].url\n",
    "\n",
    "    # Download the image from the URL\n",
    "    image_response = requests.get(image_url)\n",
    "\n",
    "    # Save the image to a file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(image_response.content)\n",
    "\n",
    "    print(f\"Image saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe984122-a1ae-43fc-a380-b528db75f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "prompt=\"Futuristic game scene of a war zone.\"\n",
    "filename = \"data/test_images/dreamvideo_image.jpg\"\n",
    "generate_and_save_image(prompt, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd5940f-bf3c-465c-92fa-176cba5fffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "prompt=prompt[:117]\n",
    "test_data = f\"data/test_images/dreamvideo_image.jpg|||{prompt}\"\n",
    "file_path = 'custom_list_dreamvideo.txt'\n",
    "# Open the file in write mode ('w') which will create the file if it doesn't exist\n",
    "with open(file_path, 'w') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e226c1a-3429-4c3e-93b2-2190f3d34999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329b832-498a-4987-960f-293b67b4051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-17 20:34:51,558] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [256, 256], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [1], 'sample_fps': [8], 'vid_dataset': {'type': 'VideoBaseDataset', 'data_list': [], 'max_words': 1000, 'resolution': [448, 256]}, 'img_dataset': {'type': 'ImageCustomDataset', 'data_list': ['data/custom/train/img_dog2.txt'], 'max_words': 6, 'resolution': [256, 256], 'data_dir_list': ['data/images/custom/dog2'], 'vit_resolution': [224, 224], 'placeholder_strings': ['*']}, 'batch_sizes': {'1': 4, '4': 4, '8': 4, '16': 4}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'linear_sd', 'schedule_param': {'num_timesteps': 1000, 'init_beta': 0.00085, 'last_beta': 0.012, 'zero_terminal_snr': False}, 'mean_type': 'eps', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_T2VBase', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'default_fps': 8, 'misc_dropout': 0.4}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPCustomEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 1, 'decoder_bs': 8, 'alpha': 0.7, 'save_ckp_interval': 200, 'warmup_steps': 10, 'decay_mode': 'none', 'use_ema': False, 'load_from': None, 'Pretrain': {'type': 'pretrain_dreamvideo', 'fix_weight': False, 'grad_scale': 0.2, 'resume_checkpoint': 'models/model_scope_v1-5_0632000.pth', 'sd_keys_path': 'data/stable_diffusion_image_key_temporal_attention_x1.json', 'fix_spatial_weight': True, 'fix_temporal_weight': True}, 'viz_interval': 200, 'visual_train': {'type': 'VisualTrainDreamVideo', 'partial_keys': [['y']], 'use_offset_noise': True, 'guide_scale': 9.0, 'infer_with_custom_text': True, 'data_list': ['data/custom/preview/subject_dog2.txt'], 'data_dir_list': ['data/images/custom/dog2']}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 10, 'log_dir': 'workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1', 'seed': 8888, 'negative_prompt': '', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'train_dreamvideo_entrance', 'embedmanager': {'type': 'EmbeddingManager', 'placeholder_strings': ['*'], 'initializer_words': ['dog'], 'per_image_tokens': False, 'num_vectors_per_token': 1, 'progressive_words': False}, 'use_textInversion': True, 'freeze_text_embedding': False, 'fix_spatial_weight': True, 'fix_temporal_weight': True, 'train_adapter': False, 'use_clip_adapter_condition': False, 'use_mask_diffusion': True, 'gen_frames': 32, 'sample_preview': True, 'noise_strength': 0.1, 'use_random_seed': False, 'cfg_file': 'configs/dreamvideo/subjectLearning/dog2_subjectLearning_step1.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': [], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/log.txt'}\n",
      "[2024-05-17 20:34:51,559] INFO: Save all the file in to dir workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1\n",
      "[2024-05-17 20:34:51,559] INFO: Going into dreamvideo training on 0 gpu\n",
      "[2024-05-17 20:34:51,565] INFO: Currnt worker with max_frames=1, batch_size=4, sample_fps=8\n",
      "[2024-05-17 20:34:51,621] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 20:34:58,050] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 20:35:01,089] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 20:35:09,624] INFO: Keys in model not matched: []\n",
      "[2024-05-17 20:35:09,624] INFO: Keys in checkpoint not matched: []\n",
      "[2024-05-17 20:35:09,642] INFO: Successfully load step 0 model from models/model_scope_v1-5_0632000.pth\n",
      "[2024-05-17 20:35:09,643] INFO: load a fixed model with 823M parameters\n",
      "[2024-05-17 20:35:19,468] INFO: Step: 0/3000 Loss: 0.042 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-17 20:35:19,468] INFO: GPU Memory used 24.50 GB\n",
      "[2024-05-17 20:36:32,001] INFO: There are 1 videos for inference.\n",
      "[2024-05-17 20:36:32,079] INFO: GPU Memory used 24.35 GB\n",
      "[2024-05-17 20:36:53,669] INFO: Save textual inversion embedding to workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0000000.pth\n",
      "[2024-05-17 20:37:07,419] INFO: Step: 10/3000 Loss: 0.058 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:37:15,342] INFO: Step: 20/3000 Loss: 0.114 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:37:23,261] INFO: Step: 30/3000 Loss: 0.021 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:37:31,230] INFO: Step: 40/3000 Loss: 0.059 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:37:39,131] INFO: Step: 50/3000 Loss: 0.103 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:37:47,087] INFO: Step: 60/3000 Loss: 0.033 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:37:54,999] INFO: Step: 70/3000 Loss: 0.044 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:02,989] INFO: Step: 80/3000 Loss: 0.058 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:10,939] INFO: Step: 90/3000 Loss: 0.029 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:18,883] INFO: Step: 100/3000 Loss: 0.146 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:26,828] INFO: Step: 110/3000 Loss: 0.047 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:34,792] INFO: Step: 120/3000 Loss: 0.033 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:42,718] INFO: Step: 130/3000 Loss: 0.084 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:50,673] INFO: Step: 140/3000 Loss: 0.141 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:38:58,579] INFO: Step: 150/3000 Loss: 0.063 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:39:06,502] INFO: Step: 160/3000 Loss: 0.070 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:39:14,432] INFO: Step: 170/3000 Loss: 0.131 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:39:22,368] INFO: Step: 180/3000 Loss: 0.046 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:39:30,319] INFO: Step: 190/3000 Loss: 0.134 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:39:38,227] INFO: Step: 200/3000 Loss: 0.071 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:39:38,228] INFO: GPU Memory used 11.87 GB\n",
      "[2024-05-17 20:40:37,343] INFO: There are 1 videos for inference.\n",
      "[2024-05-17 20:40:37,423] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:40:55,745] INFO: Save textual inversion embedding to workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0000200.pth\n",
      "[2024-05-17 20:41:03,894] INFO: Step: 210/3000 Loss: 0.084 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:41:12,088] INFO: Step: 220/3000 Loss: 0.092 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:41:20,319] INFO: Step: 230/3000 Loss: 0.076 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:41:28,684] INFO: Step: 240/3000 Loss: 0.104 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:41:36,937] INFO: Step: 250/3000 Loss: 0.097 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:41:45,156] INFO: Step: 260/3000 Loss: 0.033 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:41:53,272] INFO: Step: 270/3000 Loss: 0.097 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:01,323] INFO: Step: 280/3000 Loss: 0.194 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:09,434] INFO: Step: 290/3000 Loss: 0.071 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:17,488] INFO: Step: 300/3000 Loss: 0.057 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:25,623] INFO: Step: 310/3000 Loss: 0.013 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:33,622] INFO: Step: 320/3000 Loss: 0.040 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:41,478] INFO: Step: 330/3000 Loss: 0.031 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:49,225] INFO: Step: 340/3000 Loss: 0.061 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:42:56,997] INFO: Step: 350/3000 Loss: 0.091 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:43:04,803] INFO: Step: 360/3000 Loss: 0.175 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:43:12,561] INFO: Step: 370/3000 Loss: 0.080 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:43:20,307] INFO: Step: 380/3000 Loss: 0.091 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:43:28,087] INFO: Step: 390/3000 Loss: 0.096 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:43:35,879] INFO: Step: 400/3000 Loss: 0.077 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:43:35,880] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:44:35,192] INFO: There are 1 videos for inference.\n",
      "[2024-05-17 20:44:35,271] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:44:53,617] INFO: Save textual inversion embedding to workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0000400.pth\n",
      "[2024-05-17 20:45:01,752] INFO: Step: 410/3000 Loss: 0.096 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:45:09,894] INFO: Step: 420/3000 Loss: 0.071 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:45:18,022] INFO: Step: 430/3000 Loss: 0.031 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:45:26,171] INFO: Step: 440/3000 Loss: 0.011 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:45:34,261] INFO: Step: 450/3000 Loss: 0.023 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:45:42,350] INFO: Step: 460/3000 Loss: 0.041 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:45:50,447] INFO: Step: 470/3000 Loss: 0.060 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:45:58,529] INFO: Step: 480/3000 Loss: 0.054 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:46:06,621] INFO: Step: 490/3000 Loss: 0.141 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:46:14,739] INFO: Step: 500/3000 Loss: 0.081 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:46:22,838] INFO: Step: 510/3000 Loss: 0.099 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:46:31,000] INFO: Step: 520/3000 Loss: 0.032 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:46:39,166] INFO: Step: 530/3000 Loss: 0.031 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:46:47,230] INFO: Step: 540/3000 Loss: 0.015 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:46:55,396] INFO: Step: 550/3000 Loss: 0.141 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:47:03,539] INFO: Step: 560/3000 Loss: 0.056 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:47:11,666] INFO: Step: 570/3000 Loss: 0.021 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:47:19,784] INFO: Step: 580/3000 Loss: 0.125 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:47:27,971] INFO: Step: 590/3000 Loss: 0.031 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:47:36,186] INFO: Step: 600/3000 Loss: 0.014 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:47:36,187] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:48:35,299] INFO: There are 1 videos for inference.\n",
      "[2024-05-17 20:48:35,378] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:48:53,664] INFO: Save textual inversion embedding to workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0000600.pth\n",
      "[2024-05-17 20:49:01,899] INFO: Step: 610/3000 Loss: 0.076 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:49:09,944] INFO: Step: 620/3000 Loss: 0.020 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:49:18,074] INFO: Step: 630/3000 Loss: 0.154 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:49:26,057] INFO: Step: 640/3000 Loss: 0.026 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:49:34,005] INFO: Step: 650/3000 Loss: 0.025 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:49:41,746] INFO: Step: 660/3000 Loss: 0.103 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:49:49,503] INFO: Step: 670/3000 Loss: 0.035 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:49:57,242] INFO: Step: 680/3000 Loss: 0.066 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:50:05,078] INFO: Step: 690/3000 Loss: 0.079 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:50:12,764] INFO: Step: 700/3000 Loss: 0.055 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:50:20,483] INFO: Step: 710/3000 Loss: 0.039 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:50:28,286] INFO: Step: 720/3000 Loss: 0.022 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:50:36,068] INFO: Step: 730/3000 Loss: 0.036 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:50:44,206] INFO: Step: 740/3000 Loss: 0.016 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:50:52,341] INFO: Step: 750/3000 Loss: 0.134 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:51:00,452] INFO: Step: 760/3000 Loss: 0.032 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:51:08,633] INFO: Step: 770/3000 Loss: 0.062 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:51:16,814] INFO: Step: 780/3000 Loss: 0.025 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:51:25,017] INFO: Step: 790/3000 Loss: 0.023 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:51:33,204] INFO: Step: 800/3000 Loss: 0.051 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:51:33,206] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:52:32,501] INFO: There are 1 videos for inference.\n",
      "[2024-05-17 20:52:32,580] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:52:50,934] INFO: Save textual inversion embedding to workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0000800.pth\n",
      "[2024-05-17 20:52:59,030] INFO: Step: 810/3000 Loss: 0.066 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:53:07,122] INFO: Step: 820/3000 Loss: 0.043 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:53:15,232] INFO: Step: 830/3000 Loss: 0.037 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:53:23,303] INFO: Step: 840/3000 Loss: 0.044 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:53:31,426] INFO: Step: 850/3000 Loss: 0.092 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:53:39,514] INFO: Step: 860/3000 Loss: 0.115 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:53:47,696] INFO: Step: 870/3000 Loss: 0.012 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:53:55,851] INFO: Step: 880/3000 Loss: 0.039 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:54:03,978] INFO: Step: 890/3000 Loss: 0.130 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:54:12,169] INFO: Step: 900/3000 Loss: 0.042 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:54:20,297] INFO: Step: 910/3000 Loss: 0.039 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:54:28,431] INFO: Step: 920/3000 Loss: 0.105 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:54:36,569] INFO: Step: 930/3000 Loss: 0.067 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:54:44,671] INFO: Step: 940/3000 Loss: 0.109 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:54:52,836] INFO: Step: 950/3000 Loss: 0.108 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:55:00,967] INFO: Step: 960/3000 Loss: 0.033 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:55:09,082] INFO: Step: 970/3000 Loss: 0.077 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:55:17,167] INFO: Step: 980/3000 Loss: 0.028 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:55:25,291] INFO: Step: 990/3000 Loss: 0.018 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:55:33,406] INFO: Step: 1000/3000 Loss: 0.047 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:55:33,408] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:56:32,413] INFO: There are 1 videos for inference.\n",
      "[2024-05-17 20:56:32,491] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 20:56:50,708] INFO: Save textual inversion embedding to workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0001000.pth\n",
      "[2024-05-17 20:56:58,508] INFO: Step: 1010/3000 Loss: 0.168 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:57:06,198] INFO: Step: 1020/3000 Loss: 0.032 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:57:14,015] INFO: Step: 1030/3000 Loss: 0.067 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:57:22,097] INFO: Step: 1040/3000 Loss: 0.128 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:57:30,264] INFO: Step: 1050/3000 Loss: 0.051 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:57:38,449] INFO: Step: 1060/3000 Loss: 0.090 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:57:46,606] INFO: Step: 1070/3000 Loss: 0.111 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:57:54,730] INFO: Step: 1080/3000 Loss: 0.157 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:58:02,897] INFO: Step: 1090/3000 Loss: 0.019 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:58:11,088] INFO: Step: 1100/3000 Loss: 0.055 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:58:19,317] INFO: Step: 1110/3000 Loss: 0.040 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:58:27,515] INFO: Step: 1120/3000 Loss: 0.025 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:58:35,795] INFO: Step: 1130/3000 Loss: 0.039 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:58:44,064] INFO: Step: 1140/3000 Loss: 0.138 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:58:52,272] INFO: Step: 1150/3000 Loss: 0.043 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:59:00,530] INFO: Step: 1160/3000 Loss: 0.145 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:59:08,740] INFO: Step: 1170/3000 Loss: 0.053 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:59:17,057] INFO: Step: 1180/3000 Loss: 0.082 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:59:25,246] INFO: Step: 1190/3000 Loss: 0.056 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:59:33,429] INFO: Step: 1200/3000 Loss: 0.096 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 20:59:33,430] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 21:00:32,597] INFO: There are 1 videos for inference.\n",
      "[2024-05-17 21:00:32,676] INFO: GPU Memory used 15.77 GB\n",
      "[2024-05-17 21:00:51,025] INFO: Save textual inversion embedding to workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0001200.pth\n",
      "[2024-05-17 21:00:59,154] INFO: Step: 1210/3000 Loss: 0.048 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 21:01:07,258] INFO: Step: 1220/3000 Loss: 0.117 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 21:01:15,379] INFO: Step: 1230/3000 Loss: 0.121 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 21:01:23,483] INFO: Step: 1240/3000 Loss: 0.032 scale: 65536.0 LR: 0.0001000\n",
      "[2024-05-17 21:01:31,626] INFO: Step: 1250/3000 Loss: 0.033 scale: 65536.0 LR: 0.0001000\n"
     ]
    }
   ],
   "source": [
    "!python train_net.py --cfg configs/dreamvideo/subjectLearning/dog2_subjectLearning_step1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18c8889-9ba7-4fc4-97b2-93c0af118523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-18 14:41:44,661] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [256, 256], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [1], 'sample_fps': [8], 'vid_dataset': {'type': 'VideoBaseDataset', 'data_list': [], 'max_words': 1000, 'resolution': [448, 256]}, 'img_dataset': {'type': 'ImageCustomDataset', 'data_list': ['data/custom/train/img_dog2.txt'], 'max_words': 6, 'resolution': [256, 256], 'data_dir_list': ['data/images/custom/dog2'], 'vit_resolution': [224, 224], 'placeholder_strings': ['*']}, 'batch_sizes': {'1': 4, '4': 4, '8': 4, '16': 4}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'linear_sd', 'schedule_param': {'num_timesteps': 1000, 'init_beta': 0.00085, 'last_beta': 0.012, 'zero_terminal_snr': False}, 'mean_type': 'eps', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_DreamVideo', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'default_fps': 8, 'misc_dropout': 0.4, 'spatial_adapter_list': ['cross_attention']}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPCustomEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000, 'lr': 1e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 1, 'decoder_bs': 8, 'alpha': 0.7, 'save_ckp_interval': 100, 'warmup_steps': 10, 'decay_mode': 'none', 'use_ema': False, 'load_from': None, 'Pretrain': {'type': 'pretrain_dreamvideo', 'fix_weight': False, 'grad_scale': 0.2, 'resume_checkpoint': 'models/model_scope_v1-5_0632000.pth', 'sd_keys_path': 'data/stable_diffusion_image_key_temporal_attention_x1.json', 'fix_spatial_weight': True, 'fix_temporal_weight': True, 'train_adapter': True}, 'viz_interval': 100, 'visual_train': {'type': 'VisualTrainDreamVideo', 'partial_keys': [['y']], 'use_offset_noise': True, 'guide_scale': 9.0, 'infer_with_custom_text': True, 'data_list': ['data/custom/preview/subject_dog2.txt'], 'data_dir_list': ['data/images/custom/dog2']}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 10, 'log_dir': 'workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2', 'seed': 8888, 'negative_prompt': '', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'train_dreamvideo_entrance', 'embedmanager': {'type': 'EmbeddingManager', 'placeholder_strings': ['*'], 'initializer_words': ['dog'], 'per_image_tokens': False, 'num_vectors_per_token': 1, 'progressive_words': False}, 'use_textInversion': True, 'freeze_text_embedding': True, 'fix_spatial_weight': True, 'fix_temporal_weight': True, 'train_adapter': True, 'use_clip_adapter_condition': False, 'use_mask_diffusion': True, 'gen_frames': 32, 'sample_preview': True, 'text_embedding_path': 'workspace/dreamvideo/subjectLearningStep1/dog2_subjectLearning_step1/embeddings/text_embedding_of_*_0003000.pth', 'noise_strength': 0.1, 'use_random_seed': False, 'cfg_file': 'configs/dreamvideo/subjectLearning/dog2_subjectLearning_step2.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': [], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/log.txt'}\n",
      "[2024-05-18 14:41:44,661] INFO: Save all the file in to dir workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2\n",
      "[2024-05-18 14:41:44,661] INFO: Going into dreamvideo training on 0 gpu\n",
      "[2024-05-18 14:41:44,667] INFO: Currnt worker with max_frames=1, batch_size=4, sample_fps=8\n",
      "[2024-05-18 14:41:44,747] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-18 14:41:51,397] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-18 14:41:54,414] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-18 14:42:03,219] INFO: Keys in model not matched: ['input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'middle_block.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'middle_block.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'middle_block.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'middle_block.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias']\n",
      "[2024-05-18 14:42:03,219] INFO: Keys in checkpoint not matched: []\n",
      "[2024-05-18 14:42:03,227] INFO: train adapter param: input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,227] INFO: train adapter param: input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,227] INFO: train adapter param: input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,227] INFO: train adapter param: input_blocks.1.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,228] INFO: train adapter param: input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,228] INFO: train adapter param: input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,228] INFO: train adapter param: input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,228] INFO: train adapter param: input_blocks.2.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,228] INFO: train adapter param: input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,228] INFO: train adapter param: input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,228] INFO: train adapter param: input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,229] INFO: train adapter param: input_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,229] INFO: train adapter param: input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,229] INFO: train adapter param: input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,229] INFO: train adapter param: input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,229] INFO: train adapter param: input_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,230] INFO: train adapter param: input_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,231] INFO: train adapter param: middle_block.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,231] INFO: train adapter param: middle_block.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,231] INFO: train adapter param: middle_block.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,231] INFO: train adapter param: middle_block.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.3.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,233] INFO: train adapter param: output_blocks.4.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,234] INFO: train adapter param: output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,234] INFO: train adapter param: output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,234] INFO: train adapter param: output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,234] INFO: train adapter param: output_blocks.5.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,234] INFO: train adapter param: output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,235] INFO: train adapter param: output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,235] INFO: train adapter param: output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,235] INFO: train adapter param: output_blocks.6.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,235] INFO: train adapter param: output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,235] INFO: train adapter param: output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,235] INFO: train adapter param: output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,235] INFO: train adapter param: output_blocks.7.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,236] INFO: train adapter param: output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,236] INFO: train adapter param: output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,236] INFO: train adapter param: output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,236] INFO: train adapter param: output_blocks.8.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,236] INFO: train adapter param: output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,236] INFO: train adapter param: output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,236] INFO: train adapter param: output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,237] INFO: train adapter param: output_blocks.9.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,237] INFO: train adapter param: output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,237] INFO: train adapter param: output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,237] INFO: train adapter param: output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,237] INFO: train adapter param: output_blocks.10.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,238] INFO: train adapter param: output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 14:42:03,238] INFO: train adapter param: output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 14:42:03,238] INFO: train adapter param: output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 14:42:03,238] INFO: train adapter param: output_blocks.11.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 14:42:03,238] INFO: Successfully load step 0 model from models/model_scope_v1-5_0632000.pth\n",
      "[2024-05-18 14:42:03,238] INFO: load a fixed model with 823M parameters\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "[2024-05-18 14:42:17,496] INFO: Step: 0/1000 Loss: 0.038 scale: 65536.0 LR: 0.0000010\n",
      "[2024-05-18 14:42:17,496] INFO: GPU Memory used 18.04 GB\n",
      "[2024-05-18 14:43:32,230] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 14:43:32,307] INFO: GPU Memory used 19.75 GB\n",
      "[2024-05-18 14:43:57,836] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000000.pth\n",
      "[2024-05-18 14:43:57,943] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000000.pth\n",
      "[2024-05-18 14:44:15,432] INFO: Step: 10/1000 Loss: 0.055 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:44:22,916] INFO: Step: 20/1000 Loss: 0.109 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:44:30,390] INFO: Step: 30/1000 Loss: 0.018 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:44:37,894] INFO: Step: 40/1000 Loss: 0.057 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:44:45,421] INFO: Step: 50/1000 Loss: 0.097 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:44:52,921] INFO: Step: 60/1000 Loss: 0.031 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:45:00,385] INFO: Step: 70/1000 Loss: 0.040 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:45:07,883] INFO: Step: 80/1000 Loss: 0.054 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:45:15,382] INFO: Step: 90/1000 Loss: 0.025 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:45:22,859] INFO: Step: 100/1000 Loss: 0.143 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:45:22,860] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:46:22,598] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 14:46:22,673] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:46:41,254] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000100.pth\n",
      "[2024-05-18 14:46:41,333] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000100.pth\n",
      "[2024-05-18 14:46:48,970] INFO: Step: 110/1000 Loss: 0.106 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:46:56,561] INFO: Step: 120/1000 Loss: 0.045 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:04,160] INFO: Step: 130/1000 Loss: 0.047 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:11,718] INFO: Step: 140/1000 Loss: 0.023 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:19,398] INFO: Step: 150/1000 Loss: 0.050 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:27,067] INFO: Step: 160/1000 Loss: 0.005 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:34,754] INFO: Step: 170/1000 Loss: 0.113 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:42,461] INFO: Step: 180/1000 Loss: 0.065 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:50,189] INFO: Step: 190/1000 Loss: 0.089 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:57,804] INFO: Step: 200/1000 Loss: 0.084 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:47:57,805] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:48:57,806] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 14:48:57,882] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:49:16,513] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000200.pth\n",
      "[2024-05-18 14:49:16,589] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000200.pth\n",
      "[2024-05-18 14:49:24,245] INFO: Step: 210/1000 Loss: 0.149 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:49:31,914] INFO: Step: 220/1000 Loss: 0.049 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:49:39,554] INFO: Step: 230/1000 Loss: 0.033 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:49:47,162] INFO: Step: 240/1000 Loss: 0.056 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:49:54,841] INFO: Step: 250/1000 Loss: 0.030 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:50:02,516] INFO: Step: 260/1000 Loss: 0.046 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:50:10,103] INFO: Step: 270/1000 Loss: 0.052 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:50:17,712] INFO: Step: 280/1000 Loss: 0.128 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:50:25,407] INFO: Step: 290/1000 Loss: 0.066 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:50:33,077] INFO: Step: 300/1000 Loss: 0.046 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:50:33,077] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:51:33,139] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 14:51:33,214] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:51:51,857] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000300.pth\n",
      "[2024-05-18 14:51:51,924] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000300.pth\n",
      "[2024-05-18 14:51:59,595] INFO: Step: 310/1000 Loss: 0.138 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:52:07,206] INFO: Step: 320/1000 Loss: 0.053 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:52:14,823] INFO: Step: 330/1000 Loss: 0.026 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:52:22,405] INFO: Step: 340/1000 Loss: 0.035 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:52:30,051] INFO: Step: 350/1000 Loss: 0.076 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:52:37,672] INFO: Step: 360/1000 Loss: 0.082 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:52:45,252] INFO: Step: 370/1000 Loss: 0.023 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:52:52,890] INFO: Step: 380/1000 Loss: 0.147 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:53:00,493] INFO: Step: 390/1000 Loss: 0.016 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:53:08,138] INFO: Step: 400/1000 Loss: 0.071 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:53:08,139] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:54:08,311] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 14:54:08,386] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:54:27,051] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000400.pth\n",
      "[2024-05-18 14:54:27,117] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000400.pth\n",
      "[2024-05-18 14:54:34,803] INFO: Step: 410/1000 Loss: 0.073 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:54:42,505] INFO: Step: 420/1000 Loss: 0.060 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:54:50,156] INFO: Step: 430/1000 Loss: 0.141 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:54:57,857] INFO: Step: 440/1000 Loss: 0.018 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:55:05,581] INFO: Step: 450/1000 Loss: 0.098 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:55:13,309] INFO: Step: 460/1000 Loss: 0.012 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:55:20,953] INFO: Step: 470/1000 Loss: 0.095 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:55:28,572] INFO: Step: 480/1000 Loss: 0.043 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:55:36,128] INFO: Step: 490/1000 Loss: 0.067 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:55:43,753] INFO: Step: 500/1000 Loss: 0.030 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:55:43,754] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:56:43,913] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 14:56:43,989] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:57:02,648] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000500.pth\n",
      "[2024-05-18 14:57:02,716] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000500.pth\n",
      "[2024-05-18 14:57:10,357] INFO: Step: 510/1000 Loss: 0.050 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:57:18,048] INFO: Step: 520/1000 Loss: 0.027 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:57:25,663] INFO: Step: 530/1000 Loss: 0.053 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:57:33,336] INFO: Step: 540/1000 Loss: 0.029 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:57:40,964] INFO: Step: 550/1000 Loss: 0.061 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:57:48,641] INFO: Step: 560/1000 Loss: 0.016 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:57:56,352] INFO: Step: 570/1000 Loss: 0.122 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:58:04,028] INFO: Step: 580/1000 Loss: 0.065 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:58:11,747] INFO: Step: 590/1000 Loss: 0.114 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:58:19,393] INFO: Step: 600/1000 Loss: 0.040 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:58:19,394] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:59:19,513] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 14:59:19,591] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 14:59:38,247] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000600.pth\n",
      "[2024-05-18 14:59:38,314] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000600.pth\n",
      "[2024-05-18 14:59:45,972] INFO: Step: 610/1000 Loss: 0.016 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 14:59:53,568] INFO: Step: 620/1000 Loss: 0.174 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:01,237] INFO: Step: 630/1000 Loss: 0.049 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:08,984] INFO: Step: 640/1000 Loss: 0.101 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:16,602] INFO: Step: 650/1000 Loss: 0.048 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:24,311] INFO: Step: 660/1000 Loss: 0.016 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:31,991] INFO: Step: 670/1000 Loss: 0.145 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:39,631] INFO: Step: 680/1000 Loss: 0.025 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:47,172] INFO: Step: 690/1000 Loss: 0.029 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:54,779] INFO: Step: 700/1000 Loss: 0.006 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:00:54,779] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:01:54,920] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 15:01:54,996] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:02:13,647] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000700.pth\n",
      "[2024-05-18 15:02:13,713] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000700.pth\n",
      "[2024-05-18 15:02:21,467] INFO: Step: 710/1000 Loss: 0.010 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:02:29,230] INFO: Step: 720/1000 Loss: 0.094 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:02:36,943] INFO: Step: 730/1000 Loss: 0.072 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:02:44,607] INFO: Step: 740/1000 Loss: 0.042 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:02:52,278] INFO: Step: 750/1000 Loss: 0.103 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:02:59,962] INFO: Step: 760/1000 Loss: 0.074 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:03:07,635] INFO: Step: 770/1000 Loss: 0.142 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:03:15,287] INFO: Step: 780/1000 Loss: 0.011 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:03:22,889] INFO: Step: 790/1000 Loss: 0.093 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:03:30,604] INFO: Step: 800/1000 Loss: 0.053 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:03:30,605] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:04:30,738] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 15:04:30,813] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:04:49,466] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000800.pth\n",
      "[2024-05-18 15:04:49,666] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000800.pth\n",
      "[2024-05-18 15:04:57,307] INFO: Step: 810/1000 Loss: 0.119 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:04,954] INFO: Step: 820/1000 Loss: 0.010 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:12,605] INFO: Step: 830/1000 Loss: 0.101 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:20,164] INFO: Step: 840/1000 Loss: 0.042 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:27,796] INFO: Step: 850/1000 Loss: 0.023 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:35,452] INFO: Step: 860/1000 Loss: 0.087 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:43,093] INFO: Step: 870/1000 Loss: 0.078 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:50,721] INFO: Step: 880/1000 Loss: 0.028 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:05:58,324] INFO: Step: 890/1000 Loss: 0.027 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:06:05,946] INFO: Step: 900/1000 Loss: 0.023 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:06:05,947] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:07:06,084] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 15:07:06,160] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:07:24,819] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000900.pth\n",
      "[2024-05-18 15:07:24,892] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00000900.pth\n",
      "[2024-05-18 15:07:32,597] INFO: Step: 910/1000 Loss: 0.053 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:07:40,248] INFO: Step: 920/1000 Loss: 0.052 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:07:47,895] INFO: Step: 930/1000 Loss: 0.069 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:07:55,530] INFO: Step: 940/1000 Loss: 0.090 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:08:03,180] INFO: Step: 950/1000 Loss: 0.046 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:08:10,844] INFO: Step: 960/1000 Loss: 0.078 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:08:18,580] INFO: Step: 970/1000 Loss: 0.043 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:08:26,203] INFO: Step: 980/1000 Loss: 0.022 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:08:33,835] INFO: Step: 990/1000 Loss: 0.026 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:08:41,511] INFO: Step: 1000/1000 Loss: 0.073 scale: 65536.0 LR: 0.0000100\n",
      "[2024-05-18 15:08:41,512] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:09:41,642] INFO: There are 1 videos for inference.\n",
      "[2024-05-18 15:09:41,719] INFO: GPU Memory used 18.06 GB\n",
      "[2024-05-18 15:10:00,375] INFO: Begin to Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00001000.pth\n",
      "[2024-05-18 15:10:00,475] INFO: Save model to workspace/dreamvideo/subjectLearningStep2/dog2_subjectLearning_step2/checkpoints/adapter_00001000.pth\n",
      "[2024-05-18 15:10:00,475] INFO: Congratulations! The training is completed!\n"
     ]
    }
   ],
   "source": [
    "!python train_net.py --cfg configs/dreamvideo/subjectLearning/dog2_subjectLearning_step2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "366fbf46-a756-40bb-823f-a07866c5ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-18 15:12:21,313] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [256, 256], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [32], 'sample_fps': [8], 'vid_dataset': {'type': 'VideoCustomDataset', 'data_list': ['data/custom/train/vid_carTurn.txt'], 'max_words': 1, 'resolution': [256, 256], 'data_dir_list': ['data/videos/custom'], 'vit_resolution': [224, 224], 'get_random_frame': True}, 'img_dataset': {'type': 'ImageBaseDataset', 'data_list': ['laion_400m'], 'max_words': 1000, 'resolution': [448, 256]}, 'batch_sizes': {'1': 256, '4': 4, '8': 4, '16': 4, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'linear_sd', 'schedule_param': {'num_timesteps': 1000, 'init_beta': 0.00085, 'last_beta': 0.012, 'zero_terminal_snr': False}, 'mean_type': 'eps', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.5, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_DreamVideo', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'default_fps': 8, 'misc_dropout': 0.4, 'temporal_adapter_list': ['self_attention', 'cross_attention', 'feedforward'], 'temporal_adapter_condition_dim': 1024}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPCustomEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 3000, 'lr': 1e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 1, 'decoder_bs': 8, 'alpha': 0.7, 'save_ckp_interval': 100, 'warmup_steps': 10, 'decay_mode': 'none', 'use_ema': False, 'load_from': None, 'Pretrain': {'type': 'pretrain_dreamvideo', 'fix_weight': False, 'grad_scale': 0.2, 'resume_checkpoint': 'models/model_scope_v1-5_0632000.pth', 'sd_keys_path': 'data/stable_diffusion_image_key_temporal_attention_x1.json', 'fix_spatial_weight': True, 'fix_temporal_weight': True, 'train_adapter': True}, 'viz_interval': 100, 'visual_train': {'type': 'VisualTrainDreamVideo', 'partial_keys': [['y']], 'use_offset_noise': True, 'guide_scale': 9.0, 'infer_with_custom_text': True, 'data_list': ['data/custom/preview/motion_carTurn.txt'], 'data_dir_list': ['data/images/motionReferenceImgs']}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 10, 'log_dir': 'workspace/dreamvideo/motionLearning/carTurn_motionLearning', 'seed': 8888, 'negative_prompt': '', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'train_dreamvideo_entrance', 'use_textInversion': False, 'freeze_text_embedding': False, 'fix_spatial_weight': True, 'fix_temporal_weight': True, 'train_adapter': True, 'use_clip_adapter_condition': True, 'gen_frames': 32, 'sample_preview': True, 'save_latents': True, 'noise_strength': 0.1, 'p_image_zero': 0.5, 'appearance_guide_strength_cond': 1, 'appearance_guide_strength_uncond': 1, 'use_random_seed': False, 'cfg_file': 'configs/dreamvideo/motionLearning/carTurn_motionLearning.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': [], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/dreamvideo/motionLearning/carTurn_motionLearning/log.txt'}\n",
      "[2024-05-18 15:12:21,313] INFO: Save all the file in to dir workspace/dreamvideo/motionLearning/carTurn_motionLearning\n",
      "[2024-05-18 15:12:21,313] INFO: Going into dreamvideo training on 0 gpu\n",
      "[2024-05-18 15:12:21,319] INFO: Currnt worker with max_frames=32, batch_size=1, sample_fps=8\n",
      "[2024-05-18 15:12:21,389] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-18 15:12:21,394] INFO: car_turn.mp4 get frames failed... with error: high <= 0\n",
      "[2024-05-18 15:12:27,591] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-18 15:12:31,593] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-18 15:12:41,496] INFO: Keys in model not matched: ['input_blocks.0.1.transformer_blocks.0.attn_adapter.down_linear.weight', 'input_blocks.0.1.transformer_blocks.0.attn_adapter.down_linear.bias', 'input_blocks.0.1.transformer_blocks.0.attn_adapter.up_linear.weight', 'input_blocks.0.1.transformer_blocks.0.attn_adapter.up_linear.bias', 'input_blocks.0.1.transformer_blocks.0.attn_adapter.condition_linear.weight', 'input_blocks.0.1.transformer_blocks.0.attn_adapter.condition_linear.bias', 'input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'input_blocks.0.1.transformer_blocks.0.ff_adapter.down_linear.weight', 'input_blocks.0.1.transformer_blocks.0.ff_adapter.down_linear.bias', 'input_blocks.0.1.transformer_blocks.0.ff_adapter.up_linear.weight', 'input_blocks.0.1.transformer_blocks.0.ff_adapter.up_linear.bias', 'input_blocks.0.1.transformer_blocks.0.ff_adapter.condition_linear.weight', 'input_blocks.0.1.transformer_blocks.0.ff_adapter.condition_linear.bias', 'input_blocks.1.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'input_blocks.1.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'input_blocks.1.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'input_blocks.1.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'input_blocks.1.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'input_blocks.1.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'input_blocks.1.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'input_blocks.1.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'input_blocks.1.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'input_blocks.1.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'input_blocks.1.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'input_blocks.1.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'input_blocks.2.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'input_blocks.2.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'input_blocks.2.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'input_blocks.2.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'input_blocks.2.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'input_blocks.2.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'input_blocks.2.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'input_blocks.2.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'input_blocks.2.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'input_blocks.2.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'input_blocks.2.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'input_blocks.2.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'input_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'input_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'input_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'input_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'input_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'input_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'input_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'input_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'input_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'input_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'input_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'input_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'input_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'input_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'input_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'input_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'input_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'input_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'input_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'input_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'input_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'input_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'input_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'input_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'input_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'input_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'input_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'input_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'input_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'input_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'input_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'input_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'input_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'input_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'input_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'input_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'input_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'input_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'input_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'input_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'input_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'input_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'input_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'input_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'input_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'input_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'input_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'input_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'middle_block.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'middle_block.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'middle_block.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'middle_block.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'middle_block.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'middle_block.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'middle_block.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'middle_block.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'middle_block.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'middle_block.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'middle_block.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'middle_block.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'middle_block.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'middle_block.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'middle_block.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'middle_block.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'middle_block.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'middle_block.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.3.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.3.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.3.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.3.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.3.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.3.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.3.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.3.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.3.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.3.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.3.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.3.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.6.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.6.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.6.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.6.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.6.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.6.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.6.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.6.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.6.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.6.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.6.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.6.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.9.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.9.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.9.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.9.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.9.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.9.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.9.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.9.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.9.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.9.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.9.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.9.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.10.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.10.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.10.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.10.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.10.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.10.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.10.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.10.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.10.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.10.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.10.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.10.2.transformer_blocks.0.ff_adapter.condition_linear.bias', 'output_blocks.11.2.transformer_blocks.0.attn_adapter.down_linear.weight', 'output_blocks.11.2.transformer_blocks.0.attn_adapter.down_linear.bias', 'output_blocks.11.2.transformer_blocks.0.attn_adapter.up_linear.weight', 'output_blocks.11.2.transformer_blocks.0.attn_adapter.up_linear.bias', 'output_blocks.11.2.transformer_blocks.0.attn_adapter.condition_linear.weight', 'output_blocks.11.2.transformer_blocks.0.attn_adapter.condition_linear.bias', 'output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight', 'output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias', 'output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight', 'output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias', 'output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight', 'output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias', 'output_blocks.11.2.transformer_blocks.0.ff_adapter.down_linear.weight', 'output_blocks.11.2.transformer_blocks.0.ff_adapter.down_linear.bias', 'output_blocks.11.2.transformer_blocks.0.ff_adapter.up_linear.weight', 'output_blocks.11.2.transformer_blocks.0.ff_adapter.up_linear.bias', 'output_blocks.11.2.transformer_blocks.0.ff_adapter.condition_linear.weight', 'output_blocks.11.2.transformer_blocks.0.ff_adapter.condition_linear.bias']\n",
      "[2024-05-18 15:12:41,499] INFO: Keys in checkpoint not matched: []\n",
      "[2024-05-18 15:12:41,502] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,502] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,502] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,502] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,502] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,502] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.0.1.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,503] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.1.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,504] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.2.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,505] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,506] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,507] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,508] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,509] INFO: train adapter param: input_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,510] INFO: train adapter param: middle_block.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,511] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,511] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,511] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.3.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,512] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,513] INFO: train adapter param: output_blocks.4.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,514] INFO: train adapter param: output_blocks.5.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,515] INFO: train adapter param: output_blocks.6.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,516] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.7.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,517] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.8.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,518] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.9.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,519] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,520] INFO: train adapter param: output_blocks.10.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.cross_attn_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.ff_adapter.down_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.ff_adapter.down_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.ff_adapter.up_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.ff_adapter.up_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.ff_adapter.condition_linear.weight\n",
      "[2024-05-18 15:12:41,521] INFO: train adapter param: output_blocks.11.2.transformer_blocks.0.ff_adapter.condition_linear.bias\n",
      "[2024-05-18 15:12:41,521] INFO: Successfully load step 0 model from models/model_scope_v1-5_0632000.pth\n",
      "[2024-05-18 15:12:41,521] INFO: load a fixed model with 823M parameters\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/VGen/utils/registry.py\", line 67, in build_from_config\n",
      "    return req_type_entry(**cfg)\n",
      "  File \"/root/VGen/tools/train/train_dreamvideo_entrance.py\", line 61, in train_dreamvideo_entrance\n",
      "    worker(0, cfg)\n",
      "  File \"/root/VGen/tools/train/train_dreamvideo_entrance.py\", line 230, in worker\n",
      "    batch = next(rank_iter)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1333, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1359, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 543, in reraise\n",
      "    raise exception\n",
      "ValueError: Caught ValueError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/VGen/tools/datasets/video_custom_dataset.py\", line 61, in __getitem__\n",
      "    raise e\n",
      "  File \"/root/VGen/tools/datasets/video_custom_dataset.py\", line 53, in __getitem__\n",
      "    ref_frame, vit_frame, video_data, caption = self._get_video_data(data_dir, file_path)\n",
      "  File \"/root/VGen/tools/datasets/video_custom_dataset.py\", line 108, in _get_video_data\n",
      "    ref_idx = np.random.randint(0, len(frame_list))\n",
      "  File \"numpy/random/mtrand.pyx\", line 782, in numpy.random.mtrand.RandomState.randint\n",
      "  File \"numpy/random/_bounded_integers.pyx\", line 1334, in numpy.random._bounded_integers._rand_int64\n",
      "ValueError: high <= 0\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/VGen/train_net.py\", line 18, in <module>\n",
      "    ENGINE.build(dict(type=cfg_update.TASK_TYPE), cfg_update=cfg_update.cfg_dict)\n",
      "  File \"/root/VGen/utils/registry.py\", line 107, in build\n",
      "    return self.build_func(*args, **kwargs, registry=self)\n",
      "  File \"/root/VGen/utils/registry_class.py\", line 7, in build_func\n",
      "    return build_from_config(cfg, registry, **kwargs)\n",
      "  File \"/root/VGen/utils/registry.py\", line 69, in build_from_config\n",
      "    raise Exception(f\"Failed to invoke function {req_type_entry}, with {e}\")\n",
      "Exception: Failed to invoke function <function train_dreamvideo_entrance at 0x7fc2e3d62050>, with Caught ValueError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/VGen/tools/datasets/video_custom_dataset.py\", line 61, in __getitem__\n",
      "    raise e\n",
      "  File \"/root/VGen/tools/datasets/video_custom_dataset.py\", line 53, in __getitem__\n",
      "    ref_frame, vit_frame, video_data, caption = self._get_video_data(data_dir, file_path)\n",
      "  File \"/root/VGen/tools/datasets/video_custom_dataset.py\", line 108, in _get_video_data\n",
      "    ref_idx = np.random.randint(0, len(frame_list))\n",
      "  File \"numpy/random/mtrand.pyx\", line 782, in numpy.random.mtrand.RandomState.randint\n",
      "  File \"numpy/random/_bounded_integers.pyx\", line 1334, in numpy.random._bounded_integers._rand_int64\n",
      "ValueError: high <= 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !python train_net.py --cfg configs/dreamvideo/motionLearning/carTurn_motionLearning.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced340d-fd42-4c60-bb22-5d2528eee08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py --cfg configs/dreamvideo/infer/subject_dog2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b2069-ae28-4c13-b2ea-a5e75e0cc6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42ffe6-f73b-4dbf-a497-752c167c64d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28271d5d-1182-4857-a84b-f413c5dcccb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
