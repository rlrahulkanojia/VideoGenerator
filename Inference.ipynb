{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bd12f-dc88-4342-84f4-cbcbd68e336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai pillow requests\n",
    "# !pip install openai==1.3.3\n",
    "# !pip install simplejson\n",
    "# !pip install opencv-python\n",
    "# !pip install open-clip-torch\n",
    "# !apt-get update && apt-get install ffmpeg libsm6 libxext6  -y\n",
    "# !pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# !pip install -r requirements.txt # -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install modelscope\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66090f-2665-4cf9-af40-2aa46327277a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ba690-3547-483b-96cc-2e041a47e83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3dce9-f7fa-4057-9cd5-5622871547a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "api_key = \"sk-proj-pKaugqFY1WJ19hSq71lrT3BlbkFJVxwwYAMxPfQGX7G3eUOi\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=api_key\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb5987-abea-4dec-bfe7-283fd2eb849f",
   "metadata": {},
   "source": [
    "#### Function to generate required images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93aef5-5da3-4bd5-a803-19f78bdd016f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abd1cc-4a7b-4635-90dc-fa1f442308d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512f240-30a4-4b31-9889-25d7b94de912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_image(prompt, filename):\n",
    "    \"\"\"\n",
    "    Generates an image from a prompt using OpenAI's API and saves it locally.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The prompt to generate the image from.\n",
    "    - filename (str): The local filename to save the image.\n",
    "    \"\"\"\n",
    "    # Call the OpenAI API to generate the image\n",
    "    response = OpenAI().images.generate(\n",
    "      model=\"dall-e-3\",\n",
    "      prompt=prompt,\n",
    "      size=\"1024x1024\",\n",
    "      quality=\"standard\",\n",
    "      n=1,\n",
    "    )\n",
    "\n",
    "    # Get the image URL from the response\n",
    "    image_url = dict(response)['data'][0].url\n",
    "\n",
    "    # Download the image from the URL\n",
    "    image_response = requests.get(image_url)\n",
    "\n",
    "    # Save the image to a file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(image_response.content)\n",
    "\n",
    "    print(f\"Image saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73798172-bdbf-49c0-aee4-2e26e1127d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "Image_prompt=\"A horse running on plains, beautiful sunlight behind it.\"\n",
    "filename = \"data/test_images/test.jpg\"\n",
    "generate_and_save_image(Image_prompt, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ba974-dcb0-4ff5-8953-ce139885f3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d6e13-d8ca-4db3-9c32-02ee396d0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_prompt=\"A horse running on plains, beautiful sunlight behind it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c717a71-4bc2-4b81-8d98-9f38de0865d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_prompt=video_prompt[:117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db741777-cca4-45e9-8caf-d667899419f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = f\"data/test_images/test.jpg|||{video_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e824260-c1df-4a18-b82f-cadbd0ca7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2b85e-7d3b-41e0-9fc0-c26b648f1d5e",
   "metadata": {},
   "source": [
    "#### Creating Custom test case with shorter video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139a5d7-2ff9-436e-856d-c34ad07da0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'custom_list.txt'\n",
    "# Open the file in write mode ('w') which will create the file if it doesn't exist\n",
    "with open(file_path, 'w') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d727e62-3f6e-499d-9428-ceda207c9293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c2d0df5-e748-4c9e-b624-970312281be4",
   "metadata": {},
   "source": [
    "##### Running the script ( Model: i2vgen_xl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016d2c2-27e5-4cca-b6fd-ebdab7f63046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following code is gonna generate the 4 samples of the gives prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe4774-27e3-4ddd-bf6b-d64beb6a9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py --cfg configs/i2vgen_xl_infer.yaml  \\\n",
    "        test_list_path custom_list.txt \\\n",
    "        test_model models/i2vgen_xl_00854500.pth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0046a9-b2b4-4a88-8b72-e7dc189b11d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c47114-8d5d-4e09-a44d-8227621e7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def save_last_frame(video_path, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Capture video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Couldn't open video file.\")\n",
    "        return\n",
    "\n",
    "    last_frame = None\n",
    "\n",
    "    # Read through the video\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        last_frame = frame\n",
    "\n",
    "    # Save the last frame\n",
    "    if last_frame is not None:\n",
    "        output_path = os.path.join(output_folder, 'test.jpg')\n",
    "        cv2.imwrite(output_path, last_frame)\n",
    "        print(f\"Last frame saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No frames to save.\")\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28344d-40f4-4fd9-83e6-f1840f44ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following cell is gonna replace the input image from generated video last image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec65124-87ae-47de-bf39-3dc1505a3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "video_path = f'workspace/experiments/custom_list/test_01_00_{video_prompt.replace(\" \",\"_\").replace(\",\",\"\")}_01.mp4'\n",
    "output_folder = 'data/test_images/'\n",
    "video_path = \"workspace/experiments/custom_list/test_01_00_Focus_on_a_young_woman_around_25_with_short_brown_hair_dressed_in_casual_work_attire_rushing_through_the_crowd_03.mp4\"\n",
    "save_last_frame(video_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70b281-3e98-4637-9d80-6c8b72218f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42655bc5-a47a-4de2-a2c1-06714c3837f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8e098-a61c-4f04-bb9d-87af0a41b568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9af88f49-7925-495f-b95e-9c29aa246cdf",
   "metadata": {},
   "source": [
    "#### Running script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7596c4-9e11-40b7-8e7f-6b29ca005b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py --cfg configs/dreamvideo/infer/examples/motion_carTurn.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b298fe3-34ad-4d33-b898-e96b810ffa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bebf7-2f4e-40fa-8d65-89756285ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "model_dir = snapshot_download('iic/dreamvideo-t2v', cache_dir='models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3964a22-d8d0-4e55-b046-e9c4f378ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./models/iic/dreamvideo-t2v/* ./models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f5129-1f4d-4f5a-8e2b-857215f00672",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67361ab2-27c6-4b31-a1b8-4eda60fd6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abebf2b-dab7-4eed-bfb3-a1352ca0c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17276539-f3fe-48b3-8dbc-3b124d26d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = snapshot_download('damo/I2VGen-XL', cache_dir='models/', revision='v1.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e906e1-0fad-40ec-a7bf-fc60727a8814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a718e1-cd5a-4067-8a62-b1e15cbd3d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "035cf14a-f6b4-49d4-964e-67b928a529f9",
   "metadata": {},
   "source": [
    "## Longer Video Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08678a7-d844-4d04-ac93-1c32a9227387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "api_key = \"sk-proj-pKaugqFY1WJ19hSq71lrT3BlbkFJVxwwYAMxPfQGX7G3eUOi\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=api_key\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ace3c-4b9f-480d-a83a-e37b9966757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_image(prompt, filename):\n",
    "    \"\"\"\n",
    "    Generates an image from a prompt using OpenAI's API and saves it locally.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The prompt to generate the image from.\n",
    "    - filename (str): The local filename to save the image.\n",
    "    \"\"\"\n",
    "    # Call the OpenAI API to generate the image\n",
    "    response = OpenAI().images.generate(\n",
    "      model=\"dall-e-3\",\n",
    "      prompt=prompt,\n",
    "      size=\"1024x1024\",\n",
    "      quality=\"standard\",\n",
    "      n=1,\n",
    "    )\n",
    "\n",
    "    # Get the image URL from the response\n",
    "    image_url = dict(response)['data'][0].url\n",
    "\n",
    "    # Download the image from the URL\n",
    "    image_response = requests.get(image_url)\n",
    "\n",
    "    # Save the image to a file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(image_response.content)\n",
    "\n",
    "    print(f\"Image saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365bd36-de62-48ae-88b8-e8fa3c89c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "Image_prompt=\"An Egyptian woman looking terrified inside a dimly lit pyramid, with torches casting flickering shadows on the walls. The scene is cinematic and mystical, with a 32K UHD resolution. The style is inspired by Pixar, blending realism with a touch of animation magic to convey her emotions vividly.\"\n",
    "filename = \"data/test_images/test.jpg\"\n",
    "generate_and_save_image(Image_prompt, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b7e4f-b861-4113-8549-7cdd565d6734",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_prompt=\"An Egyptian woman looking terrified inside a dimly lit pyramid, with torches casting flickering shadows on the walls.\"\n",
    "video_prompt=video_prompt[:117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964d547-3fa7-49ce-a9ba-d98d9d69119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = f\"data/test_images/test.jpg|||{video_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a456c0c-2845-4092-a72c-d8533b15f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'custom_list_longer.txt'\n",
    "# Open the file in write mode ('w') which will create the file if it doesn't exist\n",
    "with open(file_path, 'w') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0ecc9-063d-48d7-9862-ae60119201de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removed old runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6df53-41b5-4161-b6c1-d1fc1440f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf workspace/experiments/custom_list_longer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3619d66-208e-4372-8dbc-5e817ad1c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Command and its arguments as a list\n",
    "command = [\n",
    "    'python', 'inference.py',\n",
    "    '--cfg', 'configs/i2vgen_xl_infer.yaml',\n",
    "    'test_list_path', 'custom_list_longer.txt',\n",
    "    'test_model', 'models/i2vgen_xl_00854500.pth'\n",
    "]\n",
    "\n",
    "# Number of times to run the command\n",
    "num_runs = 5\n",
    "\n",
    "for i in range(num_runs):\n",
    "    # Execute the command\n",
    "    subprocess.run(command)\n",
    "    \n",
    "    # Print completion message\n",
    "    print(f\"Run {i+1} of the script completed\")\n",
    "\n",
    "    # Specify the directory\n",
    "    directory = 'workspace/experiments/custom_list_longer/'\n",
    "    \n",
    "    # Get list of files in the directory\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and 'log' not in f]\n",
    "    \n",
    "    \n",
    "    # Find the latest file\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    new_name = os.path.join(directory, f'{i+1}.mp4')\n",
    "    \n",
    "    # Rename the latest file\n",
    "    os.rename(latest_file, new_name)\n",
    "    \n",
    "    print(f'Renamed \"{latest_file}\" to \"{new_name}\"')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2929d18-dd12-4032-9812-bcdc4cc032de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the videos\n",
    "folder_path = \"workspace/experiments/custom_list_longer\"\n",
    "\n",
    "# Get list of video files in the folder\n",
    "video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n",
    "\n",
    "# Sort video files based on their names (assuming they are named numerically)\n",
    "video_files.sort()\n",
    "\n",
    "# Initialize an empty list to store video frames\n",
    "frames = []\n",
    "\n",
    "# Read each video and store frames\n",
    "for video_file in video_files:\n",
    "    print(video_file)\n",
    "    video_path = os.path.join(folder_path, video_file)\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while True:\n",
    "        success, frame = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Release video capture object after reading the video\n",
    "    video_capture.release()\n",
    "\n",
    "# Concatenate frames vertically (assuming all videos have the same resolution\n",
    "\n",
    "# Write the concatenated video to a file\n",
    "output_path = \"concatenated_video.mp4\"\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'xvid') # Specify the codec\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # for MP4 codec\n",
    "height, width, _ = frames[0].shape  # Get the dimensions from the first frame\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 10, (width, height))\n",
    "\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "# Release the VideoWriter\n",
    "out.release()\n",
    "\n",
    "print(\"Concatenated video saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b6b79-9dd3-477c-8a8b-c0c9e4492e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video in ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f6045-af76-4fe6-9889-9b3f3b1c3cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ee492-86e1-41d3-aa22-58cd741f89ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5f8fb-ed1b-4db5-ad28-7861c9ef4efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e4d4d39-ad94-4e7f-b43d-0d6d61056c67",
   "metadata": {},
   "source": [
    "### Bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b3a686-5bca-45bf-a316-b3d0c00cfb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "api_key = \"sk-proj-pKaugqFY1WJ19hSq71lrT3BlbkFJVxwwYAMxPfQGX7G3eUOi\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=api_key\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c9643-214c-4bf6-be1f-e9ff83107e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, prompt):\n",
    "        self.name = \"Generator\"\n",
    "        self.prompt = prompt\n",
    "        self.file_path = 'custom_list_loop.txt'\n",
    "        self.image_name = \"data/test_images/test.jpg\"\n",
    "        self.command = [\n",
    "                    'python', 'inference.py',\n",
    "                    '--cfg', 'configs/i2vgen_xl_infer.yaml',\n",
    "                    'test_list_path', self.file_path,\n",
    "                    'test_model', 'models/i2vgen_xl_00854500.pth'\n",
    "                ]\n",
    "\n",
    "\n",
    "    def image_generator(self):\n",
    "        \"\"\"\n",
    "        Generates an image from a prompt using OpenAI's API and saves it locally.\n",
    "    \n",
    "        Parameters:\n",
    "        - prompt (str): The prompt to generate the image from.\n",
    "        - filename (str): The local filename to save the image.\n",
    "        \"\"\"\n",
    "        # Call the OpenAI API to generate the image\n",
    "        response = OpenAI().images.generate(\n",
    "          model=\"dall-e-3\",\n",
    "          prompt=self.prompt,\n",
    "          size=\"1024x1024\",\n",
    "          quality=\"standard\",\n",
    "          n=1,\n",
    "        )\n",
    "    \n",
    "        # Get the image URL from the response\n",
    "        image_url = dict(response)['data'][0].url\n",
    "    \n",
    "        # Download the image from the URL\n",
    "        image_response = requests.get(image_url)\n",
    "    \n",
    "        # Save the image to a file\n",
    "        with open(self.image_name, 'wb') as file:\n",
    "            file.write(image_response.content)\n",
    "    \n",
    "        print(f\"Image saved as {self.image_name}\")\n",
    "\n",
    "    def create_input_list(self):\n",
    "        test_data = f\"data/test_images/test.jpg|||{self.prompt}\"\n",
    "        # Open the file in write mode ('w') which will create the file if it doesn't exist\n",
    "        with open(self.file_path, 'w') as file:\n",
    "            # Write the string to the file\n",
    "            file.write(test_data)\n",
    "\n",
    "\n",
    "    def save_last_frame(self, video_path, output_folder='data/test_images/'):\n",
    "        # Ensure the output folder exists\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "    \n",
    "        # Capture video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Couldn't open video file.\")\n",
    "            return\n",
    "    \n",
    "        last_frame = None\n",
    "    \n",
    "        # Read through the video\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            last_frame = frame\n",
    "    \n",
    "        # Save the last frame\n",
    "        if last_frame is not None:\n",
    "            output_path = os.path.join(output_folder, 'test.jpg')\n",
    "            cv2.imwrite(output_path, last_frame)\n",
    "            print(f\"Last frame saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No frames to save.\")\n",
    "    \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Generating Image from Dalle-3\")\n",
    "        self.image_generator()\n",
    "        print(\"Creating Input list\")\n",
    "        self.create_input_list()\n",
    "        print(\"Runnning video generation\")\n",
    "        subprocess.run(self.command)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cadb96-290f-4230-a77a-6d3ea53376a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47032e8-cca8-46b2-8255-9eaad059eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = open(\"prompts_list.txt\").readlines()\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt : {prompt}\")\n",
    "    generator = Generator(prompt)\n",
    "    generator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c378ed-c90b-4b68-80a3-23a161a3b39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506740f-e519-4d73-9d60-87b55056b22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70662879-9811-499f-a336-d9e33ca01b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class Generator9sec:\n",
    "\n",
    "    def __init__(self, prompt):\n",
    "        self.name = \"Generator\"\n",
    "        self.prompt = prompt\n",
    "        self.file_path = 'custom_list_longer.txt'\n",
    "        self.image_name = \"data/test_images/test.jpg\"\n",
    "        self.command = [\n",
    "                    'python', 'inference.py',\n",
    "                    '--cfg', 'configs/i2vgen_xl_infer.yaml',\n",
    "                    'test_list_path', self.file_path,\n",
    "                    'test_model', 'models/i2vgen_xl_00854500.pth'\n",
    "                ]\n",
    "\n",
    "\n",
    "    def image_generator(self):\n",
    "        \"\"\"\n",
    "        Generates an image from a prompt using OpenAI's API and saves it locally.\n",
    "    \n",
    "        Parameters:\n",
    "        - prompt (str): The prompt to generate the image from.\n",
    "        - filename (str): The local filename to save the image.\n",
    "        \"\"\"\n",
    "        # Call the OpenAI API to generate the image\n",
    "        response = OpenAI().images.generate(\n",
    "          model=\"dall-e-3\",\n",
    "          prompt=self.prompt,\n",
    "          size=\"1024x1024\",\n",
    "          quality=\"standard\",\n",
    "          n=1,\n",
    "        )\n",
    "    \n",
    "        # Get the image URL from the response\n",
    "        image_url = dict(response)['data'][0].url\n",
    "    \n",
    "        # Download the image from the URL\n",
    "        image_response = requests.get(image_url)\n",
    "    \n",
    "        # Save the image to a file\n",
    "        with open(self.image_name, 'wb') as file:\n",
    "            file.write(image_response.content)\n",
    "    \n",
    "        print(f\"Image saved as {self.image_name}\")\n",
    "\n",
    "    def create_input_list(self):\n",
    "        test_data = f\"data/test_images/test.jpg|||{self.prompt}\"\n",
    "        # Open the file in write mode ('w') which will create the file if it doesn't exist\n",
    "        with open(self.file_path, 'w') as file:\n",
    "            # Write the string to the file\n",
    "            file.write(test_data)\n",
    "\n",
    "\n",
    "    def save_last_frame(self, video_path, output_folder='data/test_images/'):\n",
    "        # Ensure the output folder exists\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "    \n",
    "        # Capture video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Couldn't open video file.\")\n",
    "            return\n",
    "    \n",
    "        last_frame = None\n",
    "    \n",
    "        # Read through the video\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            last_frame = frame\n",
    "    \n",
    "        # Save the last frame\n",
    "        if last_frame is not None:\n",
    "            output_path = os.path.join(output_folder, 'test.jpg')\n",
    "            cv2.imwrite(output_path, last_frame)\n",
    "            print(f\"Last frame saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No frames to save.\")\n",
    "    \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "\n",
    "    def run_iterations(self, number_of_iterations=2):\n",
    "        print(\"Generating Image from Dalle-3\")\n",
    "        self.image_generator()\n",
    "        print(\"Creating Input list\")\n",
    "        self.create_input_list()\n",
    "\n",
    "        print(\"Removing earlier generated video\")\n",
    "        subprocess.run(\"rm -rf workspace/experiments/custom_list_longer/\")\n",
    "        \n",
    "        print(\"Runnning video generation\")\n",
    "        # subprocess.run(self.command) \n",
    "\n",
    "    \n",
    "        for i in range(number_of_iterations):\n",
    "            # Execute the command\n",
    "            subprocess.run(self.command)\n",
    "            \n",
    "            # Print completion message\n",
    "            print(f\"Run {i+1} of the script completed\")\n",
    "        \n",
    "            # Specify the directory\n",
    "            directory = 'workspace/experiments/custom_list_longer/'\n",
    "            \n",
    "            # Get list of files in the directory\n",
    "            files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and 'log' not in f]\n",
    "            \n",
    "            \n",
    "            # Find the latest file\n",
    "            latest_file = max(files, key=os.path.getctime)\n",
    "            new_name = os.path.join(directory, f'{i+1}.mp4')\n",
    "            \n",
    "            # Rename the latest file\n",
    "            os.rename(latest_file, new_name)\n",
    "            \n",
    "            print(f'Renamed \"{latest_file}\" to \"{new_name}\"')    \n",
    "\n",
    "            self.save_last_frame(new_name)\n",
    "            print(f\"Saved Last Image at {new_name}\")\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "    def combine_image(self):\n",
    "        \n",
    "        # Path to the folder containing the videos\n",
    "        folder_path = \"workspace/experiments/custom_list_longer\"\n",
    "        \n",
    "        # Get list of video files in the folder\n",
    "        video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n",
    "        \n",
    "        # Sort video files based on their names (assuming they are named numerically)\n",
    "        video_files.sort()\n",
    "        \n",
    "        # Initialize an empty list to store video frames\n",
    "        frames = []\n",
    "        \n",
    "        # Read each video and store frames\n",
    "        for video_file in video_files:\n",
    "            print(video_file)\n",
    "            video_path = os.path.join(folder_path, video_file)\n",
    "            video_capture = cv2.VideoCapture(video_path)\n",
    "        \n",
    "            while True:\n",
    "                success, frame = video_capture.read()\n",
    "                if not success:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "        \n",
    "            # Release video capture object after reading the video\n",
    "            video_capture.release()\n",
    "        \n",
    "        # Concatenate frames vertically (assuming all videos have the same resolution\n",
    "        \n",
    "        # Write the concatenated video to a file\n",
    "        output_path = \"combined/concatenated_video\"+str(self.prompt)+\".mp4\"\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'xvid') # Specify the codec\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # for MP4 codec\n",
    "        height, width, _ = frames[0].shape  # Get the dimensions from the first frame\n",
    "        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 8, (width, height))\n",
    "        \n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        # Release the VideoWriter\n",
    "        out.release()\n",
    "        \n",
    "        print(\"Concatenated video saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52dd8558-eba9-46a4-920d-060a16e01e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d668d2-0031-440b-907b-9fc1f1923036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : cinematic, a girl with hair and blinking eyes in a subway station looking inspired, 4K high resolution\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 07:51:00,364] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 07:51:00,364] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 07:51:00,372] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 07:51:06,720] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 07:51:10,417] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 07:51:18,785] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 07:51:19,294] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 07:51:19,295] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||cinematic, a girl with hair and blinking eyes in a subway station looking inspired, 4K high resolution ...\n",
      "[2024-05-17 07:51:21,789] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 07:56:06,547] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_cinematic_a_girl_with_hair_and_blinking_eyes_in_a_subway_station_looking_inspired_4K_high_resolution_00.mp4:\n",
      "[2024-05-17 07:56:06,547] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_cinematic_a_girl_with_hair_and_blinking_eyes_in_a_subway_station_looking_inspired_4K_high_resolution_00.mp4\" to \"workspace/experiments/custom_list_longer/1.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/1.mp4\n",
      "[2024-05-17 07:56:08,976] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 07:56:08,977] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 07:56:08,983] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 07:56:15,041] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 07:56:18,613] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 07:56:26,985] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 07:56:27,494] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 07:56:27,494] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||cinematic, a girl with hair and blinking eyes in a subway station looking inspired, 4K high resolution ...\n",
      "[2024-05-17 07:56:30,027] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:01:17,752] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_cinematic_a_girl_with_hair_and_blinking_eyes_in_a_subway_station_looking_inspired_4K_high_resolution_00.mp4:\n",
      "[2024-05-17 08:01:17,752] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_cinematic_a_girl_with_hair_and_blinking_eyes_in_a_subway_station_looking_inspired_4K_high_resolution_00.mp4\" to \"workspace/experiments/custom_list_longer/2.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/2.mp4\n",
      "1.mp4\n",
      "2.mp4\n",
      "Concatenated video saved successfully!\n",
      "Prompt : cinematic, aerial drone flythough over seashore of the vast mountain cliffs of Scotland, strong breeze blowing hair and strong ocean waves, dolly in\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 08:01:34,502] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:01:34,502] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:01:34,509] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:01:40,559] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:01:44,157] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:01:52,550] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:01:53,060] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:01:53,060] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||cinematic, aerial drone flythough over seashore of the vast mountain cliffs of Scotland, strong breeze blowing hair and strong ocean waves, dolly in ...\n",
      "[2024-05-17 08:01:55,577] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:06:43,177] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_cinematic_aerial_drone_flythough_over_seashore_of_the_vast_mountain_cliffs_of_Scotland_strong_breeze_blowing_hair_and_strong_ocean_waves_dolly_in_00.mp4:\n",
      "[2024-05-17 08:06:43,178] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_cinematic_aerial_drone_flythough_over_seashore_of_the_vast_mountain_cliffs_of_Scotland_strong_breeze_blowing_hair_and_strong_ocean_waves_dolly_in_00.mp4\" to \"workspace/experiments/custom_list_longer/1.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/1.mp4\n",
      "[2024-05-17 08:06:45,658] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:06:45,658] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:06:45,685] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:06:51,746] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:06:55,392] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:07:03,808] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:07:04,318] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:07:04,318] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||cinematic, aerial drone flythough over seashore of the vast mountain cliffs of Scotland, strong breeze blowing hair and strong ocean waves, dolly in ...\n",
      "[2024-05-17 08:07:06,842] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:11:54,566] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_cinematic_aerial_drone_flythough_over_seashore_of_the_vast_mountain_cliffs_of_Scotland_strong_breeze_blowing_hair_and_strong_ocean_waves_dolly_in_00.mp4:\n",
      "[2024-05-17 08:11:54,566] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_cinematic_aerial_drone_flythough_over_seashore_of_the_vast_mountain_cliffs_of_Scotland_strong_breeze_blowing_hair_and_strong_ocean_waves_dolly_in_00.mp4\" to \"workspace/experiments/custom_list_longer/2.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/2.mp4\n",
      "1.mp4\n",
      "2.mp4\n",
      "Concatenated video saved successfully!\n",
      "Prompt : Astronaut sitting at control board with blinking lights\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 08:12:12,630] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:12:12,630] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:12:12,656] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:12:18,640] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:12:22,222] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:12:30,635] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:12:31,137] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:12:31,137] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||Astronaut sitting at control board with blinking lights ...\n",
      "[2024-05-17 08:12:33,666] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:17:20,064] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_Astronaut_sitting_at_control_board_with_blinking_lights_00.mp4:\n",
      "[2024-05-17 08:17:20,065] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_Astronaut_sitting_at_control_board_with_blinking_lights_00.mp4\" to \"workspace/experiments/custom_list_longer/1.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/1.mp4\n",
      "[2024-05-17 08:17:22,545] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:17:22,545] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:17:22,552] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:17:28,612] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:17:32,190] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:17:40,632] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:17:41,139] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:17:41,139] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||Astronaut sitting at control board with blinking lights ...\n",
      "[2024-05-17 08:17:43,665] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:22:30,051] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_Astronaut_sitting_at_control_board_with_blinking_lights_00.mp4:\n",
      "[2024-05-17 08:22:30,051] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_Astronaut_sitting_at_control_board_with_blinking_lights_00.mp4\" to \"workspace/experiments/custom_list_longer/2.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/2.mp4\n",
      "1.mp4\n",
      "2.mp4\n",
      "Concatenated video saved successfully!\n",
      "Prompt : cinematic, Fireworks, NYC skyline, night, futuristic city, epic realism, cinematic, epic realism,8K, highly detailed, long shot technique, dreamy vibe\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 08:22:46,730] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:22:46,730] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:22:46,738] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:22:52,808] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:22:56,447] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:23:04,831] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:23:05,345] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:23:05,345] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||cinematic, Fireworks, NYC skyline, night, futuristic city, epic realism, cinematic, epic realism,8K, highly detailed, long shot technique, dreamy vibe ...\n",
      "[2024-05-17 08:23:07,862] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:27:55,407] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_cinematic_Fireworks_NYC_skyline_night_futuristic_city_epic_realism_cinematic_epic_realism8K_highly_detailed_long_shot_technique_dreamy_vibe_00.mp4:\n",
      "[2024-05-17 08:27:55,408] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_cinematic_Fireworks_NYC_skyline_night_futuristic_city_epic_realism_cinematic_epic_realism8K_highly_detailed_long_shot_technique_dreamy_vibe_00.mp4\" to \"workspace/experiments/custom_list_longer/1.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/1.mp4\n",
      "[2024-05-17 08:27:57,881] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:27:57,882] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:27:57,888] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:28:03,902] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:28:07,451] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:28:15,831] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:28:16,338] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:28:16,338] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||cinematic, Fireworks, NYC skyline, night, futuristic city, epic realism, cinematic, epic realism,8K, highly detailed, long shot technique, dreamy vibe ...\n",
      "[2024-05-17 08:28:18,864] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:33:07,393] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_cinematic_Fireworks_NYC_skyline_night_futuristic_city_epic_realism_cinematic_epic_realism8K_highly_detailed_long_shot_technique_dreamy_vibe_00.mp4:\n",
      "[2024-05-17 08:33:07,393] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_cinematic_Fireworks_NYC_skyline_night_futuristic_city_epic_realism_cinematic_epic_realism8K_highly_detailed_long_shot_technique_dreamy_vibe_00.mp4\" to \"workspace/experiments/custom_list_longer/2.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/2.mp4\n",
      "1.mp4\n",
      "2.mp4\n",
      "Concatenated video saved successfully!\n",
      "Prompt : An exotic model wearing minimalist pop art holographic projection makeup, creating a surreal and elegant aesthetic. The holographic projections give a futuristic touch to the overall look, while the model's irreverent mood adds an element of playfulness to the image. The composition is balanced and showcases the intricate details of the makeup, with vibrant colors and intricate patterns. The image has a high-definition quality, making it visually stunning.\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 08:33:23,694] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:33:23,694] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:33:23,701] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:33:29,794] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:33:33,371] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:33:41,818] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:33:42,328] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:33:42,328] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||An exotic model wearing minimalist pop art holographic projection makeup, creating a surreal and elegant aesthetic. The holographic projections give a futuristic touch to the overall look, while the model's irreverent mood adds an element of playfulness to the image. The composition is balanced and showcases the intricate details of the makeup, with vibrant colors and intricate patterns. The image has a high-definition quality, making it visually stunning. ...\n",
      "[2024-05-17 08:33:44,857] INFO: GPU Memory used 16.78 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-17 08:38:32,871] INFO: Step: save text or video error with [Errno 32] Broken pipe\n",
      "\n",
      "FFMPEG COMMAND:\n",
      "/opt/conda/lib/python3.10/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux64-v4.2.2 -y -f rawvideo -vcodec rawvideo -s 1280x704 -pix_fmt rgb24 -r 8.00 -i - -an -vcodec libx264 -pix_fmt yuv420p -crf 10 -v warning /root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4\n",
      "\n",
      "FFMPEG STDERR OUTPUT:\n",
      "\n",
      "[2024-05-17 08:38:32,872] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/2.mp4\" to \"workspace/experiments/custom_list_longer/1.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/1.mp4\n",
      "[2024-05-17 08:38:35,380] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:38:35,380] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:38:35,387] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:38:41,398] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:38:44,941] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:38:53,324] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:38:53,832] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:38:53,832] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||An exotic model wearing minimalist pop art holographic projection makeup, creating a surreal and elegant aesthetic. The holographic projections give a futuristic touch to the overall look, while the model's irreverent mood adds an element of playfulness to the image. The composition is balanced and showcases the intricate details of the makeup, with vibrant colors and intricate patterns. The image has a high-definition quality, making it visually stunning. ...\n",
      "[2024-05-17 08:38:56,360] INFO: GPU Memory used 16.78 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4: File name too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-17 08:43:44,228] INFO: Step: save text or video error with [Errno 32] Broken pipe\n",
      "\n",
      "FFMPEG COMMAND:\n",
      "/opt/conda/lib/python3.10/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux64-v4.2.2 -y -f rawvideo -vcodec rawvideo -s 1280x704 -pix_fmt rgb24 -r 8.00 -i - -an -vcodec libx264 -pix_fmt yuv420p -crf 10 -v warning /root/VGen/workspace/experiments/custom_list_longer/test_01_00_An_exotic_model_wearing_minimalist_pop_art_holographic_projection_makeup_creating_a_surreal_and_elegant_aesthetic_The_holographic_projections_give_a_futuristic_touch_to_the_overall_look_while_the_models_irreverent_mood_adds_an_element_of_playfulness_to_the_image_The_composition_is_balanced_and_showcases_the_intricate_details_of_the_makeup_with_vibrant_colors_and_intricate_patterns_The_image_has_a_highdefinition_quality_making_it_visually_stunning_00.mp4\n",
      "\n",
      "FFMPEG STDERR OUTPUT:\n",
      "\n",
      "[2024-05-17 08:43:44,228] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/1.mp4\" to \"workspace/experiments/custom_list_longer/2.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/2.mp4\n",
      "2.mp4\n",
      "Concatenated video saved successfully!\n",
      "Prompt : food shot, a perfect burger in a bun with cheese and lettuce, macro shot, rotating shot, dolly in\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 08:43:59,146] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:43:59,147] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:43:59,153] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:44:05,217] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:44:08,805] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:44:17,148] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:44:17,657] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:44:17,657] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||food shot, a perfect burger in a bun with cheese and lettuce, macro shot, rotating shot, dolly in ...\n",
      "[2024-05-17 08:44:20,191] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:49:08,511] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_food_shot_a_perfect_burger_in_a_bun_with_cheese_and_lettuce_macro_shot_rotating_shot_dolly_in_00.mp4:\n",
      "[2024-05-17 08:49:08,511] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_food_shot_a_perfect_burger_in_a_bun_with_cheese_and_lettuce_macro_shot_rotating_shot_dolly_in_00.mp4\" to \"workspace/experiments/custom_list_longer/1.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/1.mp4\n",
      "[2024-05-17 08:49:10,936] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:49:10,936] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:49:10,943] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:49:17,217] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:49:20,808] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:49:29,184] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:49:29,688] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:49:29,688] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||food shot, a perfect burger in a bun with cheese and lettuce, macro shot, rotating shot, dolly in ...\n",
      "[2024-05-17 08:49:32,214] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-17 08:54:20,709] INFO: Save video to dir workspace/experiments/custom_list_longer/test_01_00_food_shot_a_perfect_burger_in_a_bun_with_cheese_and_lettuce_macro_shot_rotating_shot_dolly_in_00.mp4:\n",
      "[2024-05-17 08:54:20,710] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/test_01_00_food_shot_a_perfect_burger_in_a_bun_with_cheese_and_lettuce_macro_shot_rotating_shot_dolly_in_00.mp4\" to \"workspace/experiments/custom_list_longer/2.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/2.mp4\n",
      "1.mp4\n",
      "2.mp4\n",
      "Concatenated video saved successfully!\n",
      "Prompt : Cinematic medium shot of the wise goddess Athena, dressed in flowing white robes, standing beside the male Greek god. They exchange a knowing glance, hinting at their shared wisdom. Mythological drama, soft lighting, 4k, sharp, Arri Alexa, depth of field, highly detailed.\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 08:54:37,690] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:54:37,690] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:54:37,697] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:54:43,698] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:54:47,290] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 08:54:55,711] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 08:54:56,221] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 08:54:56,221] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||Cinematic medium shot of the wise goddess Athena, dressed in flowing white robes, standing beside the male Greek god. They exchange a knowing glance, hinting at their shared wisdom. Mythological drama, soft lighting, 4k, sharp, Arri Alexa, depth of field, highly detailed. ...\n",
      "[2024-05-17 08:54:58,747] INFO: GPU Memory used 16.78 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-17 08:59:46,883] INFO: Step: save text or video error with [Errno 32] Broken pipe\n",
      "\n",
      "FFMPEG COMMAND:\n",
      "/opt/conda/lib/python3.10/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux64-v4.2.2 -y -f rawvideo -vcodec rawvideo -s 1280x704 -pix_fmt rgb24 -r 8.00 -i - -an -vcodec libx264 -pix_fmt yuv420p -crf 10 -v warning /root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4\n",
      "\n",
      "FFMPEG STDERR OUTPUT:\n",
      "\n",
      "[2024-05-17 08:59:46,884] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/2.mp4\" to \"workspace/experiments/custom_list_longer/1.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/1.mp4\n",
      "[2024-05-17 08:59:49,324] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 08:59:49,324] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 08:59:49,331] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 08:59:55,409] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 08:59:59,034] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 09:00:07,526] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 09:00:08,032] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 09:00:08,032] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||Cinematic medium shot of the wise goddess Athena, dressed in flowing white robes, standing beside the male Greek god. They exchange a knowing glance, hinting at their shared wisdom. Mythological drama, soft lighting, 4k, sharp, Arri Alexa, depth of field, highly detailed. ...\n",
      "[2024-05-17 09:00:10,553] INFO: GPU Memory used 16.78 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n",
      "/root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4: File name too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-17 09:04:58,720] INFO: Step: save text or video error with [Errno 32] Broken pipe\n",
      "\n",
      "FFMPEG COMMAND:\n",
      "/opt/conda/lib/python3.10/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux64-v4.2.2 -y -f rawvideo -vcodec rawvideo -s 1280x704 -pix_fmt rgb24 -r 8.00 -i - -an -vcodec libx264 -pix_fmt yuv420p -crf 10 -v warning /root/VGen/workspace/experiments/custom_list_longer/test_01_00_Cinematic_medium_shot_of_the_wise_goddess_Athena_dressed_in_flowing_white_robes_standing_beside_the_male_Greek_god_They_exchange_a_knowing_glance_hinting_at_their_shared_wisdom_Mythological_drama_soft_lighting_4k_sharp_Arri_Alexa_depth_of_field_highly_detailed_00.mp4\n",
      "\n",
      "FFMPEG STDERR OUTPUT:\n",
      "\n",
      "[2024-05-17 09:04:58,720] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/custom_list_longer/1.mp4\" to \"workspace/experiments/custom_list_longer/2.mp4\"\n",
      "Last frame saved to data/test_images/test.jpg\n",
      "Saved Last Image at workspace/experiments/custom_list_longer/2.mp4\n",
      "2.mp4\n",
      "Concatenated video saved successfully!\n",
      "Prompt : Golden retriever puppy playing in a sunny meadow\n",
      "\n",
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/test.jpg\n",
      "Creating Input list\n",
      "Runnning video generation\n",
      "[2024-05-17 09:05:15,113] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/custom_list_longer', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'custom_list_longer.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'custom_list_longer.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/custom_list_longer/log_00.txt'}\n",
      "[2024-05-17 09:05:15,113] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-17 09:05:15,120] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-17 09:05:21,250] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-17 09:05:24,827] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-17 09:05:33,273] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-17 09:05:33,773] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-17 09:05:33,773] INFO: [0]/[1] Begin to sample data/test_images/test.jpg|||Golden retriever puppy playing in a sunny meadow ...\n",
      "[2024-05-17 09:05:36,305] INFO: GPU Memory used 16.78 GB\n"
     ]
    }
   ],
   "source": [
    "prompts = open(\"prompts_list.txt\").readlines()\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt : {prompt}\")\n",
    "    generator = Generator9sec(prompt)\n",
    "    generator.run_iterations()\n",
    "    generator.combine_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c7798-fcf8-4c87-a40d-9534de4cf823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb3de4-1a0f-4da1-a120-a0589ce9777f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1185fcf1-45d0-4637-a05a-72e81704caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the videos\n",
    "folder_path = \"workspace/experiments/custom_list_longer\"\n",
    "\n",
    "# Get list of video files in the folder\n",
    "video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n",
    "\n",
    "# Sort video files based on their names (assuming they are named numerically)\n",
    "video_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e71ad58-9a59-4c38-b31e-0540e0090509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.mp4\n",
      "2.mp4\n"
     ]
    }
   ],
   "source": [
    "# Read each video and store frames\n",
    "frames = []\n",
    "for video_file in video_files:\n",
    "    print(video_file)\n",
    "    video_path = os.path.join(folder_path, video_file)\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the frame rate of the original video\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    while True:\n",
    "        success, frame = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Release video capture object after reading the video\n",
    "    video_capture.release()\n",
    "\n",
    "# Write the concatenated video to a file\n",
    "# output_path = \"concatenated_video.avi\"\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'avc1')  # for avc1 codec\n",
    "# height, width, _ = frames[0].shape  # Get the dimensions from the first frame\n",
    "# out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# for frame in frames:\n",
    "#     out.write(frame)\n",
    "\n",
    "# # Release the VideoWriter\n",
    "# out.release()\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a8c62f5-da21-4a59-82e1-7fb3ca09285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the concatenated video to a file\n",
    "output_path = \"concatenated_video.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # for XVID codec\n",
    "height, width, _ = frames[0].shape  # Get the dimensions from the first frame\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "# Release the VideoWriter\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36b8281d-0ddd-4436-bd71-b894ea1ad5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 1280, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56bbc7a7-eecb-4b03-bcd4-e88d9599d558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d23ceae7-4baf-43cd-b6a8-48130870a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or use 'XVID'\n",
    "height, width, layers = frames[0].shape\n",
    "video = cv2.VideoWriter('output.mp4', fourcc, 30.0, (width, height))\n",
    "\n",
    "# Write each frame to the output video file\n",
    "for frame in frames:\n",
    "    video.write(frame)\n",
    "\n",
    "# After all frames are written, release the video writer\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4344369-86ef-4c0f-83c8-a9adcad78d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0df32a8-53f1-4316-91bb-e66d5b29a097",
   "metadata": {},
   "source": [
    "### 18 secs video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94656f8a-5fb7-465f-8269-a5fae866c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vyom_utils import Generator_9secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bcc6a1-33ee-4e4a-94fb-69f64c633bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = open(\"vyom_prompts.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0003075f-9d41-4686-8840-9ff96c7f338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Sunny lake with mountain ranges. There is a blue lake as well\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93261725-36d1-431b-8c3c-f1b836a5fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Image from Dalle-3\n",
      "Image saved as data/test_images/vyom_9sec.jpg\n",
      "Creating Input list\n",
      "Removing earlier generated video\n",
      "Folder not created yet.\n",
      "Runnning video generation\n",
      "[2024-05-18 20:55:15,194] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/vyom_9sec_list', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'vyom_9sec_list.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'vyom_9sec_list.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/vyom_9sec_list/log_00.txt'}\n",
      "[2024-05-18 20:55:15,195] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-18 20:55:15,202] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-18 20:55:21,241] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-18 20:55:25,026] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-18 20:55:33,519] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-18 20:55:34,046] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-18 20:55:34,046] INFO: [0]/[1] Begin to sample data/test_images/vyom_9sec.jpg|||Sunny lake with mountain ranges. There is a blue lake as well ...\n",
      "[2024-05-18 20:55:36,546] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-18 21:00:19,685] INFO: Save video to dir workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4:\n",
      "[2024-05-18 21:00:19,686] INFO: Congratulations! The inference is completed!\n",
      "Run 1 of the script completed\n",
      "Renamed \"workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4\" to \"workspace/experiments/vyom_9sec_list/1.mp4\"\n",
      "Last frame saved to data/test_images/vyom_9sec.jpg\n",
      "Saved Last Video at workspace/experiments/vyom_9sec_list/1.mp4\n",
      "[2024-05-18 21:00:22,201] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/vyom_9sec_list', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'vyom_9sec_list.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'vyom_9sec_list.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/vyom_9sec_list/log_00.txt'}\n",
      "[2024-05-18 21:00:22,201] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-18 21:00:22,207] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-18 21:00:28,291] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-18 21:00:32,054] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-18 21:00:40,508] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-18 21:00:41,034] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-18 21:00:41,034] INFO: [0]/[1] Begin to sample data/test_images/vyom_9sec.jpg|||Sunny lake with mountain ranges. There is a blue lake as well ...\n",
      "[2024-05-18 21:00:43,562] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-18 21:05:28,863] INFO: Save video to dir workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4:\n",
      "[2024-05-18 21:05:28,863] INFO: Congratulations! The inference is completed!\n",
      "Run 2 of the script completed\n",
      "Renamed \"workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4\" to \"workspace/experiments/vyom_9sec_list/2.mp4\"\n",
      "Last frame saved to data/test_images/vyom_9sec.jpg\n",
      "Saved Last Video at workspace/experiments/vyom_9sec_list/2.mp4\n",
      "[2024-05-18 21:05:31,344] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/vyom_9sec_list', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'vyom_9sec_list.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'vyom_9sec_list.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/vyom_9sec_list/log_00.txt'}\n",
      "[2024-05-18 21:05:31,345] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-18 21:05:31,351] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-18 21:05:37,468] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-18 21:05:41,171] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-18 21:05:49,626] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-18 21:05:50,156] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-18 21:05:50,156] INFO: [0]/[1] Begin to sample data/test_images/vyom_9sec.jpg|||Sunny lake with mountain ranges. There is a blue lake as well ...\n",
      "[2024-05-18 21:05:52,680] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-18 21:10:38,150] INFO: Save video to dir workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4:\n",
      "[2024-05-18 21:10:38,150] INFO: Congratulations! The inference is completed!\n",
      "Run 3 of the script completed\n",
      "Renamed \"workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4\" to \"workspace/experiments/vyom_9sec_list/3.mp4\"\n",
      "Last frame saved to data/test_images/vyom_9sec.jpg\n",
      "Saved Last Video at workspace/experiments/vyom_9sec_list/3.mp4\n",
      "[2024-05-18 21:10:40,640] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/vyom_9sec_list', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'vyom_9sec_list.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'vyom_9sec_list.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/vyom_9sec_list/log_00.txt'}\n",
      "[2024-05-18 21:10:40,640] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-18 21:10:40,647] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-18 21:10:46,688] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-18 21:10:50,397] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-18 21:10:58,878] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-18 21:10:59,398] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-18 21:10:59,399] INFO: [0]/[1] Begin to sample data/test_images/vyom_9sec.jpg|||Sunny lake with mountain ranges. There is a blue lake as well ...\n",
      "[2024-05-18 21:11:01,918] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-18 21:15:47,607] INFO: Save video to dir workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4:\n",
      "[2024-05-18 21:15:47,608] INFO: Congratulations! The inference is completed!\n",
      "Run 4 of the script completed\n",
      "Renamed \"workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4\" to \"workspace/experiments/vyom_9sec_list/4.mp4\"\n",
      "Last frame saved to data/test_images/vyom_9sec.jpg\n",
      "Saved Last Video at workspace/experiments/vyom_9sec_list/4.mp4\n",
      "[2024-05-18 21:15:50,103] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 6, 'prefetch_factor': 2, 'resolution': [1280, 704], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'frame_lens': [16, 16, 16, 16, 16, 32, 32, 32], 'sample_fps': [8, 8, 16, 16, 16, 8, 16, 16], 'vid_dataset': {'type': 'VideoDataset', 'data_list': ['data/vid_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/videos/'], 'vit_resolution': [224, 224], 'get_first_frame': True}, 'img_dataset': {'type': 'ImageDataset', 'data_list': ['data/img_list.txt'], 'max_words': 1000, 'resolution': [1280, 704], 'data_dir_list': ['data/images'], 'vit_resolution': [224, 224]}, 'batch_sizes': {'1': 32, '4': 8, '8': 4, '16': 2, '32': 1}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'cosine', 'schedule_param': {'num_timesteps': 1000, 'cosine_s': 0.008, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 50, 'use_div_loss': False, 'p_zero': 0.0, 'guide_scale': 9.0, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_I2VGen', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'upper_len': 128, 'concat_dim': 4, 'default_fps': 8}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'models/open_clip_pytorch_model.bin', 'vit_resolution': [224, 224]}, 'ema_decay': 0.9999, 'num_steps': 1000000, 'lr': 3e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 50, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': True, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.5, 'resume_checkpoint': 'models/i2vgen_xl_00854500.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 50, 'visual_train': {'type': 'VisualTrainTextImageToVideo', 'partial_keys': [['y', 'image', 'local_image', 'fps']], 'use_offset_noise': True, 'guide_scale': 9.0}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 1, 'log_dir': 'workspace/experiments/vyom_9sec_list', 'seed': 8888, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'ENABLE': True, 'DATASET': 'webvid10m', 'TASK_TYPE': 'inference_i2vgen_entrance', 'max_frames': 32, 'target_fps': 16, 'scale': 8, 'round': 1, 'batch_size': 1, 'use_zero_infer': True, 'vldm_cfg': 'configs/i2vgen_xl_train.yaml', 'test_list_path': 'vyom_9sec_list.txt', 'test_model': 'models/i2vgen_xl_00854500.pth', 'cfg_file': 'configs/i2vgen_xl_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': ['test_list_path', 'vyom_9sec_list.txt', 'test_model', 'models/i2vgen_xl_00854500.pth'], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'noise_strength': 0.1, 'gpu': 0, 'rank': 0, 'log_file': 'workspace/experiments/vyom_9sec_list/log_00.txt'}\n",
      "[2024-05-18 21:15:50,103] INFO: Going into it2v_fullid_img_text inference on 0 gpu\n",
      "[2024-05-18 21:15:50,110] INFO: Loading ViT-H-14 model config.\n",
      "[2024-05-18 21:15:56,208] INFO: Loading pretrained ViT-H-14 weights (models/open_clip_pytorch_model.bin).\n",
      "[2024-05-18 21:15:59,931] INFO: Restored from models/v2-1_512-ema-pruned.ckpt\n",
      "[2024-05-18 21:16:08,391] INFO: Load model from models/i2vgen_xl_00854500.pth with status <All keys matched successfully>\n",
      "[2024-05-18 21:16:08,906] INFO: There are 1 videos. with 1 times\n",
      "[2024-05-18 21:16:08,906] INFO: [0]/[1] Begin to sample data/test_images/vyom_9sec.jpg|||Sunny lake with mountain ranges. There is a blue lake as well ...\n",
      "[2024-05-18 21:16:11,429] INFO: GPU Memory used 16.78 GB\n",
      "[2024-05-18 21:20:57,118] INFO: Save video to dir workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4:\n",
      "[2024-05-18 21:20:57,119] INFO: Congratulations! The inference is completed!\n",
      "Run 5 of the script completed\n",
      "Renamed \"workspace/experiments/vyom_9sec_list/vyom_9sec_01_00_Sunny_lake_with_mountain_ranges_There_is_a_blue_lake_as_well_00.mp4\" to \"workspace/experiments/vyom_9sec_list/5.mp4\"\n",
      "Last frame saved to data/test_images/vyom_9sec.jpg\n",
      "Saved Last Video at workspace/experiments/vyom_9sec_list/5.mp4\n",
      "1.mp4\n",
      "2.mp4\n",
      "3.mp4\n",
      "4.mp4\n",
      "5.mp4\n",
      "Concatenated video saved successfully!\n"
     ]
    }
   ],
   "source": [
    "generator = Generator_9secs(prompt)\n",
    "generator.run_iterations(number_of_iterations=5) #Each iteration increases the video duration by 4 secs ( 32 frames )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075b3ed-950f-4451-88af-ab9ec467e2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f49ee1-9ffe-4dd2-9acc-d22fbdea12c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
